{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moaz9090/AI-based-threat-detection-system/blob/main/Moaz_GP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "J5p0YJO4LR6d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHCGIoVUifXE",
        "outputId": "75546f35-b94e-4a44-b355-67a0d1e61475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.27.1)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner --upgrade\n",
        "!pip install heatmapz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_vJkYEYKuij"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras_tuner as kt\n",
        "import matplotlib.pyplot as plt\n",
        "from fastai.tabular.all import df_shrink\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from heatmap import heatmap, corrplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DDld54Wdh7E"
      },
      "source": [
        "# data load:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YSwHWRJY7ms"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djqCz3p9hNuX"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Moaz_GP/New data/NF-UNSW-NB15-v2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLIK19LEZFNr"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk7xAQO7cPxI"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data visualization before preprocessing**"
      ],
      "metadata": {
        "id": "y_JfOPaNKojz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "corrplot(data.corr(),size_scale=800)"
      ],
      "metadata": {
        "id": "Es_5-WHzAywR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "iF5zIDTlLGD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5f17nl4enWt"
      },
      "outputs": [],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s29VVblFeq4R"
      },
      "outputs": [],
      "source": [
        "data.Label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-clFSUneyz6"
      },
      "outputs": [],
      "source": [
        "data.Attack.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyXgPYWEe1if"
      },
      "outputs": [],
      "source": [
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWerRaLefIlS"
      },
      "outputs": [],
      "source": [
        "data.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju8YO-xFgM5r"
      },
      "outputs": [],
      "source": [
        "data = data.drop(columns=['IPV4_SRC_ADDR', 'IPV4_DST_ADDR'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA2NgRADgmcQ"
      },
      "outputs": [],
      "source": [
        "data = df_shrink(data, obj2cat=False, int2uint=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpEo2BbegqYJ"
      },
      "outputs": [],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRbkomKLhG-b"
      },
      "outputs": [],
      "source": [
        "data.replace([np.inf, -np.inf], np.nan, inplace=True)    \n",
        "print(data.isna().any(axis=1).sum(), \"rows with at least one NaN to remove\")\n",
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_f8c2u5hNXp"
      },
      "outputs": [],
      "source": [
        "print(data.duplicated().sum(), \"fully duplicate rows to remove\")\n",
        "data.drop_duplicates(inplace=True)\n",
        "data.reset_index(inplace=True, drop=True)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data visualization after preprocessing**"
      ],
      "metadata": {
        "id": "_kZg6MHTK1Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "corrplot(data.corr(),size_scale=800)"
      ],
      "metadata": {
        "id": "RGjj37FTDcuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fl31hkjhVgC"
      },
      "outputs": [],
      "source": [
        "# Columns that are not necessary for training\n",
        "columns_to_remove = ['L4_SRC_PORT','L4_DST_PORT','Label','Attack']\n",
        "\n",
        "# Train test split\n",
        "train , test= train_test_split(data, test_size=0.2, shuffle=True)\n",
        "y_test = np.array(test['Label'], dtype=np.uint0)\n",
        "\n",
        "# Indices of benign and attack traffic in train data\n",
        "train_benign_idx = train['Label'] == 0\n",
        "train_attack_idx = train['Label'] == 1\n",
        "\n",
        "# Drop unnecessary columns\n",
        "train.drop(columns=columns_to_remove, axis=1, inplace=True)\n",
        "test.drop(columns=columns_to_remove, axis=1, inplace=True)\n",
        "\n",
        "# Cast to numpy array\n",
        "train_normal = train[train_benign_idx].values\n",
        "train_attack = train[train_attack_idx].values\n",
        "\n",
        "\n",
        "# Scaling\n",
        "scaler = MinMaxScaler()\n",
        "train = scaler.fit_transform(train_normal)\n",
        "train_attack = scaler.transform(train_attack)\n",
        "test = scaler.fit_transform(test.values)\n",
        "\n",
        "# Define a validation set\n",
        "train , validation = train_test_split(train, test_size=0.2)\n",
        "\n",
        "print(f'Shape train data: {train.shape}')\n",
        "print(f'Shape validation data: {validation.shape}')\n",
        "print(f'Shape test data: {test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVqrkdvBix1w"
      },
      "outputs": [],
      "source": [
        "def test_model(model, threshold_quantile, validation_benign, validation_attack, test, y_test, mae=True):\n",
        "    \n",
        "    # Evaluate the losses of the reconstructions of the validation set with benign traffic\n",
        "    val_losses = None\n",
        "    if mae:\n",
        "        # MAE loss\n",
        "        val_losses = np.mean(abs(validation_benign - model.predict(validation_benign)), axis=1)\n",
        "    else:\n",
        "        #MSE loss\n",
        "        val_losses = np.mean((validation_benign - model.predict(validation_benign))**2, axis=1)\n",
        "        \n",
        "    val_losses = pd.DataFrame({'benign' : val_losses})\n",
        "        \n",
        "    print('Statistics benign reconstruction losses:')\n",
        "    print('-'*20)\n",
        "    print(val_losses.describe())\n",
        "    \n",
        "    \n",
        "    # Evaluate the losses of the reconstructions of the validation set with attack traffic\n",
        "    attack_losses = None\n",
        "    if mae:\n",
        "        # MAE loss\n",
        "        attack_losses = np.mean(abs(validation_attack - model.predict(validation_attack)), axis=1)\n",
        "    else:\n",
        "        # MSE loss\n",
        "        attack_losses = np.mean((validation_attack - model.predict(validation_attack))**2, axis=1)\n",
        "\n",
        "    attack_losses = pd.DataFrame({'attack' : attack_losses})\n",
        "    \n",
        "    print()\n",
        "    print('Statistics attack reconstruction losses:')\n",
        "    print('-'*20)\n",
        "    print(attack_losses.describe())\n",
        "    \n",
        "    \n",
        "    # Define the threshold based on the supplied quantile\n",
        "    threshold = np.quantile(val_losses, 0.99)\n",
        "\n",
        "    test_losses = None\n",
        "    recons = model.predict(test)\n",
        "    if mae:\n",
        "        # MAE loss\n",
        "        test_losses = np.mean(abs(test - recons), axis=1)\n",
        "    else:\n",
        "        # MSE loss\n",
        "        test_losses = np.mean((test - recons)**2, axis=1)\n",
        "        \n",
        "    preds = np.array(test_losses > threshold, dtype=np.uint0)\n",
        "    \n",
        "    \n",
        "    print(f'ACCURACY:\\n\\t{accuracy_score(preds, y_test)}')\n",
        "    print(f'PRECISION:\\n\\t{precision_score(preds, y_test)}')\n",
        "    print(f'RECALL:\\n\\t{recall_score(preds, y_test)}')\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
        "    print(f'True Positives: {tp}')\n",
        "    print(f'False Positives: {fp}')\n",
        "    print(f'True Negatives: {tn}')\n",
        "    print(f'False Negatives: {fn}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKtLfv7zjvtX"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(4, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(39, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "model.fit(train, train, batch_size=128, epochs=50, validation_split=0.1, shuffle=True, callbacks=[es])\n",
        "\n",
        "print(\"TRAINED WITH LOSS 'MAE':\")\n",
        "print(\"=\"*20)\n",
        "print(\"\\tEVALUATE WITH MAE & QUANTILE 0.95:\")\n",
        "test_model(model, 0.95, validation, train_attack, test, y_test)\n",
        "print(\"\\tEVALUATE WITH MAE & QUANTILE 0.98:\")\n",
        "test_model(model, 0.98, validation, train_attack, test, y_test)\n",
        "print(\"\\tEVALUATE WITH MSE & QUANTILE 0.95:\")\n",
        "test_model(model, 0.95, validation, train_attack, test, y_test, mae=False)\n",
        "print(\"\\tEVALUATE WITH MSE & QUANTILE 0.98:\")\n",
        "test_model(model, 0.98, validation, train_attack, test, y_test, mae=False)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(4, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(39, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "model.fit(train, train, batch_size=128, epochs=50, validation_split=0.1, shuffle=True, callbacks=[es])\n",
        "\n",
        "print(\"TRAINED WITH LOSS 'MSE':\")\n",
        "print(\"=\"*20)\n",
        "print(\"\\tEVALUATE WITH MAE & QUANTILE 0.95:\")\n",
        "test_model(model, 0.95, validation, train_attack, test, y_test)\n",
        "print(\"\\tEVALUATE WITH MAE & QUANTILE 0.98:\")\n",
        "test_model(model, 0.98, validation, train_attack, test, y_test)\n",
        "print(\"\\tEVALUATE WITH MSE & QUANTILE 0.95:\")\n",
        "test_model(model, 0.95, validation, train_attack, test, y_test, mae=False)\n",
        "print(\"\\tEVALUATE WITH MSE & QUANTILE 0.98:\")\n",
        "test_model(model, 0.98, validation, train_attack, test, y_test, mae=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgjyN5p-qCGf"
      },
      "source": [
        "# Hyperparameter tuning for the simple autoencoder\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No3ff4BVj0Zr"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        32,\n",
        "        activation=hp.Choice(f'encoder_layer_1_activation', ['relu','tanh','selu','elu']),\n",
        "        activity_regularizer=tf.keras.regularizers.l2(hp.Choice(\"regularizer_1\", [1e-2,1e-3,1e-4,1e-5,1e-6]))\n",
        "    ))\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        16, \n",
        "        activation=hp.Choice(f'encoder_layer_2_activation', ['relu','tanh','selu','elu']),\n",
        "        activity_regularizer=tf.keras.regularizers.l2(hp.Choice(\"regularizer_2\", [1e-2,1e-3,1e-4,1e-5,1e-6]))\n",
        "\n",
        "    ))\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        8, \n",
        "        activation=hp.Choice(f'encoder_layer_3_activation', ['relu','tanh','selu','elu']),\n",
        "        activity_regularizer=tf.keras.regularizers.l2(hp.Choice(\"regularizer_3\", [1e-2,1e-3,1e-4,1e-5,1e-6]))\n",
        "    ))\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        4, \n",
        "        activation=hp.Choice(f'latent_dim_activation', ['relu','tanh','selu','elu']),\n",
        "        activity_regularizer=tf.keras.regularizers.l2(hp.Choice(\"regularizer_latent_dim\", [1e-2,1e-3,1e-4,1e-5,1e-6]))\n",
        "    ))\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        8, \n",
        "        activation=hp.Choice(f'decoder_layer_1_activation', ['relu','tanh','selu','elu']),\n",
        "        activity_regularizer=tf.keras.regularizers.l2(hp.Choice(\"regularizer_4\", [1e-2,1e-3,1e-4,1e-5,1e-6]))\n",
        "    ))\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        16,\n",
        "        activation=hp.Choice(f'decoder_layer_2_activation', ['relu','tanh','selu','elu']),\n",
        "        activity_regularizer=tf.keras.regularizers.l2(hp.Choice(\"regularizer_5\", [1e-2,1e-3,1e-4,1e-5,1e-6]))\n",
        "    ))\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        32,\n",
        "        activation=hp.Choice(f'decoder_layer_3_activation', ['relu','tanh','selu','elu']),\n",
        "        activity_regularizer=tf.keras.regularizers.l2(hp.Choice(\"regularizer_6\", [1e-2,1e-3,1e-4,1e-5,1e-6]))    \n",
        "    ))\n",
        "    model.add(tf.keras.layers.Dense(39, activation='sigmoid'))\n",
        "    \n",
        "    \n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='mae'\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tuner1 = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_loss\",\n",
        "    max_trials=10,\n",
        "    directory='results_tuning',\n",
        "    project_name='autoencoder'\n",
        ")\n",
        "\n",
        "# Use a subset of the training data\n",
        "idx_tuner_data = int(len(train)*0.5)\n",
        "tuner_data = train[:idx_tuner_data]\n",
        "\n",
        "tuner1.search(\n",
        "    tuner_data, tuner_data,\n",
        "    validation_split=0.1,\n",
        "    batch_size=128,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "model_1 = tuner1.get_best_models()[0]\n",
        "tuner1.results_summary(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8OrEvBJqRQM"
      },
      "source": [
        "# Train the best model on more data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj_Dm5p1qFfU"
      },
      "outputs": [],
      "source": [
        "model_1.compile(optimizer='adam', loss='mae')\n",
        "model_1.fit(train, train, epochs=50, batch_size=128, validation_split=0.1, shuffle=True, callbacks=[es])\n",
        "\n",
        "print(\"TRAINED WITH LOSS 'MAE':\")\n",
        "print(\"=\"*20)\n",
        "print(\"\\tEVALUATE WITH MAE & QUANTILE 0.95:\")\n",
        "test_model(model_1, 0.95, validation, train_attack, test, y_test)\n",
        "print(\"\\tEVALUATE WITH MAE & QUANTILE 0.98:\")\n",
        "test_model(model_1, 0.98, validation, train_attack, test, y_test)\n",
        "print(\"\\tEVALUATE WITH MSE & QUANTILE 0.95:\")\n",
        "test_model(model_1, 0.95, validation, train_attack, test, y_test, mae=False)\n",
        "print(\"\\tEVALUATE WITH MSE & QUANTILE 0.98:\")\n",
        "test_model(model_1, 0.98, validation, train_attack, test, y_test, mae=False)\n",
        "\n",
        "model_1 = tuner1.get_best_models()[0]\n",
        "model_1.compile(optimizer='adam', loss='mse')\n",
        "model_1.fit(train, train, epochs=50, batch_size=128, validation_split=0.1, shuffle=True, callbacks=[es])\n",
        "\n",
        "print(\"TRAINED WITH LOSS 'MSE':\")\n",
        "print(\"=\"*20)\n",
        "print(\"\\tEVALUATE WITH MAE & QUANTILE 0.95:\")\n",
        "test_model(model_1, 0.95, validation, train_attack, test, y_test)\n",
        "print(\"\\tEVALUATE WITH MAE & QUANTILE 0.98:\")\n",
        "test_model(model_1, 0.98, validation, train_attack, test, y_test)\n",
        "print(\"\\tEVALUATE WITH MSE & QUANTILE 0.95:\")\n",
        "test_model(model_1, 0.95, validation, train_attack, test, y_test, mae=False)\n",
        "print(\"\\tEVALUATE WITH MSE & QUANTILE 0.98:\")\n",
        "test_model(model_1, 0.98, validation, train_attack, test, y_test, mae=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvGXz-29zMWZ"
      },
      "source": [
        "# Search for a more complex architecture¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORhKPQU7u_bn"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "    model = tf.keras.Sequential()\n",
        "    hidden_layers = list()\n",
        "    \n",
        "    for i in range(hp.Int('encoder_layers', min_value=1, max_value=6, step=1)):\n",
        "        n_neurons = hp.Int(f'encoder_layer_{i}', min_value=16, max_value=39, step=2)\n",
        "        model.add(tf.keras.layers.Dense(\n",
        "                    units=n_neurons,\n",
        "                    activation=hp.Choice(f'encoder_layer_{i}_activation', ['relu','tanh']),\n",
        "                    activity_regularizer=tf.keras.regularizers.l1(hp.Choice(f'encoder_layer_{i}_regularizer', [1e-1, 1e-2, 1e-3, 1e-4]))\n",
        "                )\n",
        "        )\n",
        "        if hp.Boolean(\"dropout\"):\n",
        "            model.add(tf.keras.layers.Dropout(rate=hp.Choice(f'encoder_layer_{i}_dropout', [0.25, 0.5])))\n",
        "        \n",
        "        \n",
        "        hidden_layers.insert(0, n_neurons)\n",
        "        \n",
        "    model.add(\n",
        "        tf.keras.layers.Dense(\n",
        "            units=hp.Int('latent_dimension', min_value=4, max_value=15, step=1),\n",
        "            activation=hp.Choice(f'latent_dimension_activation', ['relu','tanh']),\n",
        "            activity_regularizer=tf.keras.regularizers.l1(hp.Choice(f'latent_dimension_regularizer', [1e-1, 1e-2, 1e-3, 1e-4]))\n",
        "        )\n",
        "    )\n",
        "    if hp.Boolean(\"dropout\"):\n",
        "            model.add(tf.keras.layers.Dropout(rate=hp.Choice(f'latent_dimension_dropout', [0.25, 0.5])))\n",
        "    \n",
        "    decoder_layer = 0\n",
        "    for neurons in hidden_layers:\n",
        "        model.add(\n",
        "            tf.keras.layers.Dense(\n",
        "                neurons,\n",
        "                activation=hp.Choice(f'decoder_layer_{decoder_layer}_activation', ['relu','tanh']),\n",
        "                activity_regularizer=tf.keras.regularizers.l1(hp.Choice(f'decoder_layer_{decoder_layer}_regularizer', [1e-1, 1e-2, 1e-3, 1e-4]))\n",
        "            )\n",
        "        )\n",
        "        if hp.Boolean(\"dropout\"):\n",
        "            model.add(tf.keras.layers.Dropout(rate=hp.Choice(f'decoder_layer_{i}_dropout', [0.25, 0.5])))\n",
        "        decoder_layer += 1\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(39, activation='sigmoid'))\n",
        "    \n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='mae'\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tuner2 = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_loss\",\n",
        "    max_trials=10,\n",
        "    directory='results_tuning',\n",
        "    project_name='autoencoder'\n",
        ")\n",
        "\n",
        "idx_tuner_data = int(len(train)*0.5)\n",
        "tuner_data = train[:idx_tuner_data]\n",
        "\n",
        "tuner2.search(\n",
        "    tuner_data, tuner_data,\n",
        "    validation_split=0.1,\n",
        "    batch_size=128,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "model_2 = tuner2.get_best_models()[0]\n",
        "tuner2.results_summary(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model_1.predict(train)\n",
        "prediction"
      ],
      "metadata": {
        "id": "CYAoqJZrUwL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.save('NIDS_model.h5')"
      ],
      "metadata": {
        "id": "Zhx1h3MOkTLm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "eebe7ae2-f08b-4296-b9d6-886370ef7cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-eaf9f4d00ab9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NIDS_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = load_model('/content/drive/MyDrive/Moaz_GP/NIDS_model.hdf5')"
      ],
      "metadata": {
        "id": "XcGAW87KDcWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GoCFe-u9Dpom"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}